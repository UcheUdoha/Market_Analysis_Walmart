{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWRkFuco98fvBJhbmh6hTL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UcheUdoha/Market_Analysis_Walmart/blob/main/Market_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2zRPD3jAH5r",
        "outputId": "0d98f30c-f46b-4277-da87-5fda05aeb251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            " User_ID                       0\n",
            "Product_ID                    0\n",
            "Gender                        0\n",
            "Age                           0\n",
            "Occupation                    0\n",
            "City_Category                 0\n",
            "Stay_In_Current_City_Years    0\n",
            "Marital_Status                0\n",
            "Product_Category              1\n",
            "Purchase                      1\n",
            "dtype: int64\n",
            "Cleaned Data Sample:\n",
            "    Gender   Age  Occupation City_Category  Stay_In_Current_City_Years  \\\n",
            "0       0  0-17          10             A                           2   \n",
            "1       0  0-17          10             A                           2   \n",
            "2       0  0-17          10             A                           2   \n",
            "3       0  0-17          10             A                           2   \n",
            "4       1   55+          16             C                           4   \n",
            "\n",
            "   Marital_Status  Product_Category  Purchase  \n",
            "0               0               3.0  0.344298  \n",
            "1               0               1.0  0.631599  \n",
            "2               0              12.0  0.052034  \n",
            "3               0              12.0  0.036680  \n",
            "4               0               8.0  0.327430  \n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/walmart.csv'  # Updated file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Check for missing values\n",
        "print(\"Missing values:\\n\", data.isnull().sum())\n",
        "\n",
        "# Step 2: Handle missing values (if any)\n",
        "# Since no missing values are apparent here, this step is for reference:\n",
        "# data['Column_Name'].fillna(value, inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical variables to numerical (if necessary)\n",
        "# Example: Encoding Gender\n",
        "data['Gender'] = data['Gender'].map({'F': 0, 'M': 1})\n",
        "\n",
        "# Step 4: Transform 'Stay_In_Current_City_Years' into numeric\n",
        "data['Stay_In_Current_City_Years'] = data['Stay_In_Current_City_Years'].replace({'4+': 4}).astype(int)\n",
        "\n",
        "# Step 5: Normalize numerical features (if applicable)\n",
        "# Example: Normalize 'Purchase' column\n",
        "scaler = MinMaxScaler()\n",
        "data['Purchase'] = scaler.fit_transform(data[['Purchase']])\n",
        "\n",
        "# Step 6: Drop unnecessary columns (if any)\n",
        "# Example: Dropping User_ID and Product_ID for customer-level analysis\n",
        "data.drop(['User_ID', 'Product_ID'], axis=1, inplace=True)\n",
        "\n",
        "# Display cleaned data\n",
        "print(\"Cleaned Data Sample:\\n\", data.head())\n",
        "\n",
        "# Save the cleaned dataset for further use\n",
        "data.to_csv('/content/cleaned_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "\n",
        "# Update the file path to the correct location of your CSV file\n",
        "file_path_second = '/content/Walmart_sales.csv'  # Changed to a likely location within Colab or similar environment\n",
        "\n",
        "# Check if the file exists at the given path\n",
        "if not os.path.exists(file_path_second):\n",
        "    # If the file is not found, raise an error with suggestions\n",
        "    raise FileNotFoundError(f\"File not found at '{file_path_second}'. \"\n",
        "                            f\"Please ensure the file exists and the path is correct. \"\n",
        "                            f\"You can also try providing the full file path or checking file permissions.\")\n",
        "\n",
        "# Load the dataset\n",
        "data_second = pd.read_csv(file_path_second)\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "metadata": {
        "id": "ffEKn4JOF-8V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "\n",
        "# Update the file path to the correct location of your CSV file\n",
        "file_path_second = '/content/Walmart_sales.csv'  # Changed to a likely location within Colab or similar environment\n",
        "\n",
        "# Check if the file exists at the given path\n",
        "if not os.path.exists(file_path_second):\n",
        "    # If the file is not found, raise an error with suggestions\n",
        "    raise FileNotFoundError(f\"File not found at '{file_path_second}'. \"\n",
        "                            f\"Please ensure the file exists and the path is correct. \"\n",
        "                            f\"You can also try providing the full file path or checking file permissions.\")\n",
        "\n",
        "# Load the dataset\n",
        "data_second = pd.read_csv(file_path_second)\n",
        "\n",
        "# Step 1: Check for missing values\n",
        "print(\"Missing values:\\n\", data_second.isnull().sum())\n",
        "\n",
        "# Step 2: Handle missing values (if any)\n",
        "# If there are missing values, fill or drop them\n",
        "data_second.fillna(method='ffill', inplace=True)  # Example: Forward fill missing values\n",
        "\n",
        "# Step 3: Convert date to datetime format\n",
        "data_second['Date'] = pd.to_datetime(data_second['Date'], format='%d-%m-%Y')\n",
        "\n",
        "# Step 4: Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "numerical_columns = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "data_second[numerical_columns] = scaler.fit_transform(data_second[numerical_columns])\n",
        "\n",
        "# Step 5: Feature Engineering (if applicable)\n",
        "# Add a new column for Year and Month\n",
        "data_second['Year'] = data_second['Date'].dt.year\n",
        "data_second['Month'] = data_second['Date'].dt.month\n",
        "\n",
        "# Drop the original 'Date' column if it's no longer needed\n",
        "data_second.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "# Step 6: Display cleaned dataset\n",
        "print(\"Cleaned Data Sample:\\n\", data_second.head())\n",
        "\n",
        "# Save the cleaned dataset\n",
        "data_second.to_csv('/content/cleaned_Walmart_sales.csv', index=False) # Updated the save path as well"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9xkRs6mGlFE",
        "outputId": "a910fdcf-daf2-43e5-e0a2-365b840b5360"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            " Store           0\n",
            "Date            0\n",
            "Weekly_Sales    0\n",
            "Holiday_Flag    0\n",
            "Temperature     0\n",
            "Fuel_Price      0\n",
            "CPI             0\n",
            "Unemployment    0\n",
            "dtype: int64\n",
            "Cleaned Data Sample:\n",
            "    Store  Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price       CPI  \\\n",
            "0      1      0.397291             0     0.434149    0.050100  0.840500   \n",
            "1      1      0.396811             1     0.396967    0.038076  0.841941   \n",
            "2      1      0.388501             0     0.410861    0.021042  0.842405   \n",
            "3      1      0.332458             0     0.476419    0.044589  0.842707   \n",
            "4      1      0.372661             0     0.475147    0.076653  0.843008   \n",
            "\n",
            "   Unemployment  Year  Month  \n",
            "0      0.405118  2010      2  \n",
            "1      0.405118  2010      2  \n",
            "2      0.405118  2010      2  \n",
            "3      0.405118  2010      2  \n",
            "4      0.405118  2010      3  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-69850e1934ed>:23: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  data_second.fillna(method='ffill', inplace=True)  # Example: Forward fill missing values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned datasets\n",
        "df1 = pd.read_csv('/content/cleaned_dataset.csv')\n",
        "df2 = pd.read_csv('/content/cleaned_Walmart_sales.csv')\n",
        "\n",
        "# Check for common columns for merging\n",
        "common_cols = list(set(df1.columns) & set(df2.columns))\n",
        "print(f\"Common columns: {common_cols}\")\n",
        "\n",
        "# Choose an appropriate common column for merging (e.g., 'Store' if available)\n",
        "merge_column = 'Store'  # Replace 'Store' with the actual common column if different\n",
        "\n",
        "# Check if the chosen merge column exists in both dataframes\n",
        "if merge_column in common_cols:\n",
        "    # Merge the dataframes on the common column\n",
        "    merged_data = pd.merge(df1, df2, on=merge_column)\n",
        "\n",
        "    # Save the merged dataset\n",
        "    merged_data.to_csv('/content/merged_dataset.csv', index=False)\n",
        "else:\n",
        "    print(f\"Error: Merge column '{merge_column}' not found in both datasets.\")\n",
        "    print(\"Please select a valid common column for merging.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y5VF1zrHBeu",
        "outputId": "a3b87335-7434-44ad-cdaf-c0a7d2590cd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common columns: []\n",
            "Error: Merge column 'Store' not found in both datasets.\n",
            "Please select a valid common column for merging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned datasets\n",
        "df1 = pd.read_csv('/content/cleaned_dataset.csv')\n",
        "df2 = pd.read_csv('/content/cleaned_Walmart_sales.csv')\n",
        "\n",
        "# Print the columns of each dataframe to identify potential merge columns\n",
        "print(\"Columns in df1:\", df1.columns)\n",
        "print(\"Columns in df2:\", df2.columns)\n",
        "\n",
        "# Assuming no common columns except for an index, perform an outer join\n",
        "merged_data = pd.merge(df1, df2, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "# Alternatively, if there's a common column like 'Store' or 'City', use it:\n",
        "# merge_column = 'Store'  # or 'City' or any other common column\n",
        "# merged_data = pd.merge(df1, df2, on=merge_column, how='outer')\n",
        "\n",
        "# Save the merged dataset\n",
        "merged_data.to_csv('/content/merged_dataset.csv', index=False)\n",
        "print(\"Datasets merged and saved to '/content/merged_dataset.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJnKved_IxSM",
        "outputId": "72bb8e46-5b6f-483d-e11a-3912df984c82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df1: Index(['Gender', 'Age', 'Occupation', 'City_Category',\n",
            "       'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category',\n",
            "       'Purchase'],\n",
            "      dtype='object')\n",
            "Columns in df2: Index(['Store', 'Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price',\n",
            "       'CPI', 'Unemployment', 'Year', 'Month'],\n",
            "      dtype='object')\n",
            "Datasets merged and saved to '/content/merged_dataset.csv'\n"
          ]
        }
      ]
    }
  ]
}